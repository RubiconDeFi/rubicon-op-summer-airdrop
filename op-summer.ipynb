{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from graphqlclient import GraphQLClient\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "import matplotlib.pyplot as plt\n",
    "from iteration_utilities import duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_total = 135000\n",
    "cutoff_timestamp = 1653339960\n",
    "ovm1_timestamp = 1636696800\n",
    "lp_percent = 0.69\n",
    "taker_percent = 0.31\n",
    "lp_scalar = 2.25\n",
    "taker_scalar = 1.5\n",
    "volume_cutoff = 10\n",
    "pool_split = {'ETH': .325, 'WBTC': .1, 'USDC': .275, 'USDT': .1, 'DAI': .15, 'SNX': .05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function to handle the calling of data from the graph \n",
    "def graph_query(api, query, variables, column, id = 'id'):\n",
    "\n",
    "    # set the graph client\n",
    "    client = GraphQLClient(api)\n",
    "\n",
    "    # set variables to get column values to populate dataframe\n",
    "    col_variables = {\"lastID\": \"\", \"amount\": 1, \"lastTimestamp\": 0}\n",
    "\n",
    "    # query the subgarph and load into dataframe\n",
    "    col_result = client.execute(query, col_variables)\n",
    "    col_data = json.loads(col_result)\n",
    "    print(col_data)\n",
    "    col_df = pd.json_normalize(col_data['data'][column])\n",
    "\n",
    "    # create and empty dataframe to append to\n",
    "    columns = list(col_df.columns)\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # now query the subgraph for the next batch of data\n",
    "    result = client.execute(query, variables)\n",
    "    data = json.loads(result)\n",
    "    update = pd.json_normalize(data['data'][column])\n",
    "\n",
    "    while update.empty != True:\n",
    "        df = df.append(update)\n",
    "        variables['lastID'] = update[id].iloc[-1]\n",
    "        result = client.execute(query, variables)\n",
    "        data = json.loads(result)\n",
    "        update = pd.json_normalize(data['data'][column]) \n",
    "\n",
    "    # reset the index\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to map asset addresses to decimals\n",
    "op_main_asset_decimals = {\n",
    "    '0x4200000000000000000000000000000000000006' : 18,\n",
    "    '0x7f5c764cbc14f9669b88837ca1490cca17c31607' : 6,\n",
    "    '0x68f180fcce6836688e9084f035309e29bf0a2095' : 8,\n",
    "    '0xda10009cbd5d07dd0cecc66161fc93d7c9000da1' : 18,\n",
    "    '0x94b008aa00579c1307b0ef2c499ad98a8ce58e58' : 6,\n",
    "    '0x8700daec35af8ff88c16bdf0418774cb3d7599b4' : 18,\n",
    "    '0x4200000000000000000000000000000000000042' : 18\n",
    "}\n",
    "    \n",
    "# a dictionary to map asset addresses to symbols\n",
    "op_main_asset_names = {\n",
    "    '0x4200000000000000000000000000000000000006' : 'ETH',\n",
    "    '0x7f5c764cbc14f9669b88837ca1490cca17c31607' : 'USDC',\n",
    "    '0x68f180fcce6836688e9084f035309e29bf0a2095' : 'WBTC',\n",
    "    '0xda10009cbd5d07dd0cecc66161fc93d7c9000da1' : 'DAI',\n",
    "    '0x94b008aa00579c1307b0ef2c499ad98a8ce58e58' : 'USDT',\n",
    "    '0x8700daec35af8ff88c16bdf0418774cb3d7599b4' : 'SNX',\n",
    "    '0x4200000000000000000000000000000000000042' : 'OP'\n",
    "}\n",
    "\n",
    "# a dictionary to map asset addresses to symbols\n",
    "op_symbols = {\n",
    "    '0x4200000000000000000000000000000000000006' : 'ETH',\n",
    "    '0x7f5c764cbc14f9669b88837ca1490cca17c31607' : 'USDC',\n",
    "    '0x68f180fcce6836688e9084f035309e29bf0a2095' : 'WBTC',\n",
    "    '0xda10009cbd5d07dd0cecc66161fc93d7c9000da1' : 'DAI',\n",
    "    '0x94b008aa00579c1307b0ef2c499ad98a8ce58e58' : 'USDT',\n",
    "    '0x8700daec35af8ff88c16bdf0418774cb3d7599b4' : 'SNX',\n",
    "    '0x4200000000000000000000000000000000000042' : 'OP',\n",
    "    '0xb0be5d911e3bd4ee2a8706cf1fac8d767a550497' : 'ETH',\n",
    "    '0x7571cc9895d8e997853b1e0a1521ebd8481aa186' : 'WBTC',\n",
    "    '0xe0e112e8f33d3f437d1f895cbb1a456836125952' : 'USDC',\n",
    "    '0x60daec2fc9d2e0de0577a5c708bcadba1458a833' : 'DAI',\n",
    "    '0xffbd695bf246c514110f5dae3fa88b8c2f42c411' : 'USDT',\n",
    "    '0xeb5f29afaaa3f44eca8559c3e8173003060e919f' : 'SNX',\n",
    "    '0x574a21fe5ea9666dbca804c9d69d8caf21d5322b' : 'OP'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/exclude_list.pkl', 'rb') as f:\n",
    "    exclude_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in lp data from ovm1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the lp data from dune query: https://dune.com/queries/1134014\n",
    "lp_data = pd.read_csv('data/dune/lp_ovm1.csv')\n",
    "\n",
    "# convert the timestamp to a unix timestamp\n",
    "lp_data['timestamp'] = pd.to_datetime(lp_data['timestamp'])\n",
    "\n",
    "# convert the timestamp to a unix timestamp\n",
    "lp_data['timestamp'] = lp_data.timestamp.values.astype(np.int64) // 10**9\n",
    "\n",
    "# for strings in the asset, user, and txn columns as a 0 to the beginning of the string\n",
    "lp_data['asset'] = lp_data['asset'].apply(lambda x: '0' +  x[1:])\n",
    "lp_data['user'] = lp_data['user'].apply(lambda x: '0' +  x[1:])\n",
    "lp_data['txn'] = lp_data['txn'].apply(lambda x: '0' +  x[1:])\n",
    "\n",
    "# convert the asset to the symbol \n",
    "lp_data['asset'] = lp_data['asset'].apply(lambda x: op_symbols[x])\n",
    "\n",
    "# convert change values to integers\n",
    "lp_data['assetChange'] = lp_data['assetChange'].map(int)\n",
    "lp_data['shareChange'] = lp_data['shareChange'].map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in snapshot lp activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the json file \n",
    "with open('data/ovm1/poolsSnapshot_ovm1.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "# flatten the json data into a pandas dataframe for deposits\n",
    "deposits_df =  pd.json_normalize(data, record_path='deposits')\n",
    "\n",
    "# flatten the json data into a pandas dataframe for withdrawals\n",
    "withdrawals_df =  pd.json_normalize(data, record_path='withdrawals')\n",
    "\n",
    "# map the shares columns to integers for later calculations\n",
    "deposits_df.sharesReceived = deposits_df.sharesReceived.map(int)\n",
    "withdrawals_df.sharesWithdrawn = withdrawals_df.sharesWithdrawn.map(int)\n",
    "\n",
    "# iterate through the deposits dataframe and convert the amount to decimals that is then stored in the dataframe\n",
    "for index, row in deposits_df.iterrows():\n",
    "    deposits_df.loc[index, 'depositedAmt'] = int(deposits_df.loc[index, 'depositedAmt']) / 10 ** (op_main_asset_decimals[row['asset']])\n",
    "\n",
    "# iterate through the withdrawals dataframe and convert the amount to decimals that is then stored in the dataframe\n",
    "for index, row in withdrawals_df.iterrows():\n",
    "    withdrawals_df.loc[index, 'amountWithdrawn'] = - 1 * int(withdrawals_df.loc[index, 'amountWithdrawn']) / 10 ** (op_main_asset_decimals[row['asset']])\n",
    "    withdrawals_df.loc[index, 'sharesWithdrawn'] = - 1 * int(withdrawals_df.loc[index, 'sharesWithdrawn'])\n",
    "\n",
    "# convert the timestamp from string to integer\n",
    "deposits_df['timestamp'] = deposits_df['timestamp'].map(int)\n",
    "withdrawals_df['timestamp'] = withdrawals_df['timestamp'].map(int)\n",
    "\n",
    "# convert the asset address to a symbol\n",
    "deposits_df['asset'] = deposits_df['asset'].map(op_main_asset_names)\n",
    "withdrawals_df['asset'] = withdrawals_df['asset'].map(op_main_asset_names)\n",
    "\n",
    "deposits_df.rename(columns={'depositor': 'user', 'depositedAmt': 'assetChange', 'sharesReceived': 'shareChange', 'id': 'txn'}, inplace=True)\n",
    "withdrawals_df.rename(columns={'withdrawer': 'user', 'amountWithdrawn': 'assetChange', 'sharesWithdrawn': 'shareChange', 'id': 'txn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add data from gap in snx lp activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    '0xB0bE5d911E3BD4Ee2A8706cF1fAc8d767A550497', # Pools\n",
    "    '0x7571CC9895D8E997853B1e0A1521eBd8481aa186',\n",
    "    '0xe0e112e8f33d3f437D1F895cbb1A456836125952',\n",
    "    '0x60daEC2Fc9d2e0de0577A5C708BcaDBA1458A833',\n",
    "    '0xfFBD695bf246c514110f5DAe3Fa88B8c2f42c411',\n",
    "    '0xeb5F29AfaaA3f44eca8559c3e8173003060e919f',\n",
    "    '0x574a21fE5ea9666DbCA804C9d69d8Caf21d5322b'\n",
    "]\n",
    "\n",
    "# lowercase the pools\n",
    "pools = [pool.lower() for pool in pools]\n",
    "\n",
    "# create a list to store the dataframes\n",
    "snx_activity = []\n",
    "\n",
    "for file in os.listdir('data/snx/dune-gap'):\n",
    "\n",
    "    # set a dataframe for each file\n",
    "    update = pd.read_csv('data/snx/dune-gap/' + file, index_col=False)\n",
    "\n",
    "    # rename some columns \n",
    "    update.rename(columns={'Txhash' : 'txn', 'UnixTimestamp' : 'timestamp', 'From' : 'from', 'To' : 'to' , 'TokenSymbol' : 'symbol', 'ContractAddress' : 'contract', 'Value' : 'value'}, inplace=True)\n",
    "\n",
    "    # now filter only for lp activity, this is any transaction where either the from, to, or contract address is in the list of pools\n",
    "    update = update[update['from'].isin(pools) | update.to.isin(pools) | update.contract.isin(pools)]\n",
    "\n",
    "    # filter for only symbols that start with bath\n",
    "    update = update[update.symbol.str.startswith('bath')]\n",
    "\n",
    "    # now drop the 'bath' from the symbol\n",
    "    update['symbol'] = update['symbol'].str.replace('bath', '')\n",
    "\n",
    "    snx_activity.append(update)\n",
    "\n",
    "snx_correction = pd.concat(snx_activity)\n",
    "snx_correction.reset_index(drop=True, inplace=True)\n",
    "\n",
    "snx_correction['value'] = snx_correction['value'].map(float)\n",
    "\n",
    "snx_correction_df = pd.DataFrame(columns=['asset', 'user', 'assetChange', 'shareChange', 'timestamp', 'txn'])\n",
    "\n",
    "# gap data fill \n",
    "for index, row in snx_correction.iterrows():\n",
    "    # if the contract is minting bathTokens\n",
    "    if row['from'] == '0x0000000000000000000000000000000000000000':\n",
    "        if row['value'] == 0:\n",
    "            sharec = 0\n",
    "        else:\n",
    "            sharec = int(snx_correction.loc[index, 'value'] * (10**18))\n",
    "        snx_correction_df = snx_correction_df.append({'asset' : row['symbol'], 'user' : row['to'], 'assetChange' : row['value'], 'shareChange' : sharec, 'timestamp' : row['timestamp'], 'txn' : row['txn']}, ignore_index=True)\n",
    "    # if the contract is burning bathTokens\n",
    "    elif row['to'] == '0x0000000000000000000000000000000000000000':\n",
    "        if row['value'] == 0:\n",
    "            sharec = 0\n",
    "        else:\n",
    "            sharec = int(-1 * snx_correction.loc[index, 'value'] * (10**18))\n",
    "        snx_correction_df = snx_correction_df.append({'asset' : row['symbol'], 'user' : row['from'], 'assetChange' : -row['value'], 'shareChange' : sharec, 'timestamp' : row['timestamp'], 'txn' : row['txn']}, ignore_index=True)\n",
    "    # if users are sending bathTokens to each other\n",
    "    elif (row['from'] != '0x0000000000000000000000000000000000000000') and row['to'] == '0x0000000000000000000000000000000000000000':\n",
    "        if row['value'] == 0:\n",
    "            sharec = 0\n",
    "        else:\n",
    "            sharec = int(snx_correction.loc[index, 'value'] * (10**18))\n",
    "        snx_correction_df = snx_correction_df.append({'asset' : row['symbol'], 'user' : row['to'], 'assetChange' : row['value'], 'shareChange' : sharec, 'timestamp' : row['timestamp'], 'txn' : row['txn']}, ignore_index=True)\n",
    "        snx_correction_df = snx_correction_df.append({'asset' : row['symbol'], 'user' : row['from'], 'assetChange' : -row['value'], 'shareChange' : (-1 * sharec), 'timestamp' : row['timestamp'], 'txn' : row['txn']}, ignore_index=True)\n",
    "\n",
    "# set values to integers\n",
    "snx_correction_df['shareChange'] = snx_correction_df['shareChange'].map(int)\n",
    "snx_correction_df['timestamp'] = snx_correction_df['timestamp'].map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in lp data from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the queries to get the data from the graph\n",
    "# define the query that is used to get the pool creation data\n",
    "deposit_query = '''\n",
    "    query allDeposits($lastID: ID, $amount: Int, $lastTimestamp: Int) {\n",
    "        depositeds(first: $amount, where: {id_gt: $lastID, timestamp_gte: $lastTimestamp}) {\n",
    "            id\n",
    "            timestamp\n",
    "            txn\n",
    "            depositor\n",
    "            asset\n",
    "            depositAmount\n",
    "            sharesReceived\n",
    "        }\n",
    "    }'''\n",
    "\n",
    "withdraw_query = '''\n",
    "    query allWithdraws($lastID: ID, $amount: Int, $lastTimestamp: Int) {\n",
    "        withdraweds(first: $amount, where: {id_gt: $lastID, timestamp_gte: $lastTimestamp}) {\n",
    "            id\n",
    "            timestamp\n",
    "            txn\n",
    "            withdrawer\n",
    "            asset\n",
    "            withdrawAmount\n",
    "            sharesBurned\n",
    "        }\n",
    "    }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to call the graph query function to get the data\n",
    "api = 'https://api.thegraph.com/subgraphs/name/denverbaumgartner/lowlevellogs'\n",
    "variables1 = {\"lastID\": \"\", \"amount\": 1000, \"lastTimestamp\": 0}\n",
    "variables2 = {\"lastID\": \"\", \"amount\": 1000, \"lastTimestamp\": 0}\n",
    "column1 = 'depositeds'\n",
    "column2 = 'withdraweds'\n",
    "ovm2_deposits_df = graph_query(api, deposit_query, variables1, column1, id = 'id')\n",
    "ovm2_withdraws_df = graph_query(api, withdraw_query, variables2, column2, id = 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get tranfer data from the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the query to get the data from the graph\n",
    "transfer_query = '''\n",
    "    query allTranferss($lastID: ID, $amount: Int, $lastTimestamp: Int, $filterString: String) {\n",
    "        transfereds(first: $amount, where: {id_gt: $lastID, timestamp_gte: $lastTimestamp, recipient_not: $filterString, sender_not: $filterString}) {\n",
    "            id\n",
    "            timestamp\n",
    "            txn\n",
    "            sender\n",
    "            recipient\n",
    "            pool\n",
    "            amount\n",
    "        }\n",
    "    }'''\n",
    "\n",
    "api = 'https://api.thegraph.com/subgraphs/name/denverbaumgartner/lowlevellogs'\n",
    "variables = {\"lastID\": \"\", \"amount\": 1000, \"lastTimestamp\": 0, \"filterString\": \"0x0000000000000000000000000000000000000000\"}\n",
    "column = 'transfereds'\n",
    "ovm2_transfers_df = graph_query(api, transfer_query, variables, column, id = 'id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary of pool names and addresses\n",
    "pool_names = {\n",
    "    '0xb0be5d911e3bd4ee2a8706cf1fac8d767a550497' : 'ETH', \n",
    "    '0x7571cc9895d8e997853b1e0a1521ebd8481aa186' : 'WBTC',\n",
    "    '0xe0e112e8f33d3f437d1f895cbb1a456836125952' : 'USDC', \n",
    "    '0x60daec2fc9d2e0de0577a5c708bcadba1458a833' : 'DAI',\n",
    "    '0xffbd695bf246c514110f5dae3fa88b8c2f42c411' : 'USDT', \n",
    "    '0xeb5f29afaaa3f44eca8559c3e8173003060e919f' : 'SNX',\n",
    "    '0x574a21fe5ea9666dbca804c9d69d8caf21d5322b' : 'OP'\n",
    "}\n",
    "\n",
    "# ['asset', 'user', 'assetChange', 'shareChange', 'timestamp', 'txn']\n",
    "ovm2_transfers = pd.DataFrame(columns=['asset', 'user', 'assetChange', 'shareChange', 'timestamp', 'txn'])\n",
    "\n",
    "# now iterate through the transfers and format to a way that could be applied to the overall dataframe\n",
    "for index, row in ovm2_transfers_df.iterrows():\n",
    "    ovm2_transfers = ovm2_transfers.append({'asset' : pool_names[row['pool']], 'user' : row['sender'], 'assetChange' : row['amount'], 'shareChange' : -1 * int(ovm2_transfers_df.loc[index, 'amount']), 'timestamp' : row['timestamp'], 'txn' : row['txn']}, ignore_index=True)\n",
    "    ovm2_transfers = ovm2_transfers.append({'asset' : pool_names[row['pool']], 'user' : row['recipient'], 'assetChange' : row['amount'], 'shareChange' : int(ovm2_transfers_df.loc[index, 'amount']), 'timestamp' : row['timestamp'], 'txn' : row['txn']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the ovm2.0 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column values to integers\n",
    "ovm2_deposits_df.depositAmount = ovm2_deposits_df.depositAmount.map(int)\n",
    "ovm2_deposits_df.sharesReceived = ovm2_deposits_df.sharesReceived.map(int)\n",
    "ovm2_withdraws_df.withdrawAmount = ovm2_withdraws_df.withdrawAmount.map(int)\n",
    "ovm2_withdraws_df.sharesBurned = ovm2_withdraws_df.sharesBurned.map(int)\n",
    "\n",
    "# iterate through the deposits dataframe and convert the amount to decimals that is then stored in the dataframe\n",
    "for index, row in ovm2_deposits_df.iterrows():\n",
    "    asset = ovm2_deposits_df.loc[index, 'asset']\n",
    "    ovm2_deposits_df.loc[index, 'depositAmount'] = ovm2_deposits_df.loc[index, 'depositAmount'] / (10 ** (op_main_asset_decimals[ovm2_deposits_df.loc[index, 'asset']]))\n",
    "\n",
    "# iterate through the withdrawals dataframe and convert the amount to decimals that is then stored in the dataframe\n",
    "for index, row in ovm2_withdraws_df.iterrows():\n",
    "    ovm2_withdraws_df.loc[index, 'withdrawAmount'] = ovm2_withdraws_df.loc[index, 'withdrawAmount'] / (10 ** (op_main_asset_decimals[ovm2_withdraws_df.loc[index, 'asset']]))\n",
    "\n",
    "# convert the asset address to a symbol\n",
    "ovm2_deposits_df['asset'] = ovm2_deposits_df['asset'].map(op_main_asset_names)\n",
    "ovm2_withdraws_df['asset'] = ovm2_withdraws_df['asset'].map(op_main_asset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean and merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to integer\n",
    "ovm2_deposits_df['timestamp'] = ovm2_deposits_df['timestamp'].map(int)\n",
    "ovm2_withdraws_df['timestamp'] = ovm2_withdraws_df['timestamp'].map(int)\n",
    "ovm2_transfers['timestamp'] = ovm2_transfers['timestamp'].map(int)\n",
    "\n",
    "# convert withdrawal values to negative values\n",
    "ovm2_withdraws_df['withdrawAmount'] = ovm2_withdraws_df['withdrawAmount'] * -1\n",
    "ovm2_withdraws_df['sharesBurned'] = ovm2_withdraws_df['sharesBurned'] * -1\n",
    "\n",
    "\n",
    "# rename the columns so they can be merged on the same columns\n",
    "ovm2_deposits_df.rename(columns={'depositor': 'user', 'depositAmount': 'assetChange', 'sharesReceived': 'shareChange'}, inplace=True)\n",
    "ovm2_withdraws_df.rename(columns={'withdrawer': 'user', 'withdrawAmount': 'assetChange', 'sharesBurned': 'shareChange'}, inplace=True)\n",
    "\n",
    "merged_df = pd.DataFrame(columns = ['asset', 'user', 'assetChange', 'shareChange', 'timestamp', 'txn'])\n",
    "\n",
    "# merge the dataframes\n",
    "merged_df = merged_df.append(ovm2_deposits_df)\n",
    "merged_df = merged_df.append(ovm2_withdraws_df)\n",
    "merged_df = merged_df.append(lp_data)\n",
    "\n",
    "# add in transfer data from the graph\n",
    "merged_df = merged_df.append(ovm2_transfers)\n",
    "\n",
    "# add in snapshot data \n",
    "merged_df = merged_df.append(deposits_df)\n",
    "merged_df = merged_df.append(withdrawals_df)\n",
    "\n",
    "# add in the snx data \n",
    "merged_df = merged_df.append(snx_correction_df)\n",
    "\n",
    "# sort by timestamp\n",
    "# sometimes there are multiple transactions within the same timestamp, in instances like this we need to make deposits appear before withdrawals \n",
    "merged_df.sort_values(by = ['timestamp', 'shareChange'], ascending = [True, False], inplace=True)\n",
    "\n",
    "# drop the id column \n",
    "merged_df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# make all user values lowercase\n",
    "merged_df['user'] = merged_df['user'].str.lower()\n",
    "\n",
    "# set the timestamp to be an integer\n",
    "merged_df['timestamp'] = merged_df['timestamp'].map(int)\n",
    "\n",
    "# if any acivity has happened after a certain timestamp, drop it \n",
    "merged_df = merged_df[merged_df['timestamp'] <= int(cutoff_timestamp)]\n",
    "\n",
    "# clean up ovm 1.0 data\n",
    "merged_df['txn'] = merged_df['txn'].apply(lambda x: x.split('-')[1] if '-' in x else x)\n",
    "\n",
    "# drop any duplicate transactions\n",
    "merged_df = merged_df.drop_duplicates(subset=['txn', 'user'], keep='first')\n",
    "\n",
    "# reset and drop the index\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any rows that have a user in the exclude_list\n",
    "merged_df = merged_df[~merged_df['user'].isin(exclude_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the file so we have it for later if needed \n",
    "merged_df.to_pickle('data/pickles/lp_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the dataframe from the pickle file\n",
    "#smerged_df = pd.read_pickle('data/pickles/lp_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split lp data by asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary that maps the assets to their corresponding subsets of the merged dataframe\n",
    "assets = list(merged_df['asset'].unique())\n",
    "asset_subsets = {}\n",
    "\n",
    "# iterate through the assets and create a dictionary of dataframes\n",
    "for asset in assets: \n",
    "    subset = merged_df[merged_df['asset'] == asset]\n",
    "\n",
    "    # reset the index \n",
    "    subset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # create a column that is the total shareCount for the pool \n",
    "    subset['totalShareCount'] = subset['shareChange'].cumsum()\n",
    "\n",
    "    asset_subsets[asset] = subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a dataframe of liquidity events and returns a dataframe representing liquidity providers relative proportion over time for a given asset\n",
    "def weighted_liquidity(history): \n",
    "    # for each user, we need to get the current amount of shares that they hold at each timestamp\n",
    "    users = list(history['user'].unique())\n",
    "    user_shares = {}\n",
    "\n",
    "    # iterate through the dataframe and get the subset of the users lp activities \n",
    "    for user in users: \n",
    "        subset = history[history['user'] == user]\n",
    "\n",
    "        # create a variable to store the users share count \n",
    "        shares = 0 \n",
    "\n",
    "        # now go through the dataframe and get the change in share count during a timestamp\n",
    "        timestamps = list(subset['timestamp'].unique())\n",
    "\n",
    "        # set a dictionary to store the share count for each timestamp\n",
    "        time_state = {}\n",
    "\n",
    "        for timestamp in timestamps: \n",
    "            # get the subset of the dataframe for the current timestamp\n",
    "            timestamp_subset = subset[subset['timestamp'] == timestamp]\n",
    "\n",
    "            # get the sum of the share change for the current timestamp\n",
    "            change = timestamp_subset['shareChange'].sum()\n",
    "            shares += change\n",
    "            time_state[timestamp] = shares\n",
    "\n",
    "        user_shares[user] = time_state\n",
    "\n",
    "    # for each timestamp we want to get the total share count for the pool \n",
    "    # within some timestamps we will have multiple transactions, so we need to sum the share changes during that period and apply that change to the previous value \n",
    "    # get all the unique timestamps\n",
    "    timestamps = list(history['timestamp'].unique())\n",
    "\n",
    "    # set the initial value of the total share count to 0\n",
    "    shareCount = 0 \n",
    "    timeCounts = {}\n",
    "    shareCounts = []\n",
    "    timeSums = {}\n",
    "\n",
    "    # iterate through the timestamps and get the sum of shareChanges at that timestamp\n",
    "    for timestamp in timestamps:\n",
    "        # get the subset of the dataframe that has the same timestamp\n",
    "        subset = history[history['timestamp'] == timestamp]\n",
    "\n",
    "        # get the sum of the share changes\n",
    "        change = subset['shareChange'].sum()\n",
    "        shareCount = shareCount + change\n",
    "\n",
    "        # add the timestamp and the shareCount to the dictionary\n",
    "        timeCounts[timestamp] = shareCount\n",
    "        shareCounts.append(shareCount)\n",
    "\n",
    "        # add the timesatmp and sum of the share changes to the dictionary\n",
    "        timeSums[timestamp] = change\n",
    "    \n",
    "    # create a data frame that contains the timestamps and the total share count\n",
    "    timestamps = list(timeCounts.keys())\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['timestamp'] = timestamps\n",
    "    df['shareCount'] = shareCounts\n",
    "\n",
    "    # go through each user and fill in the share count for each timestamp\n",
    "    for user in users: \n",
    "        df[user] = df['timestamp'].map(user_shares[user])\n",
    "    \n",
    "    # forward fill the users share count for empty timestamps\n",
    "    df = df.ffill()\n",
    "\n",
    "    # get the difference between each rows timestamps\n",
    "    df['time_range'] = df['timestamp'].diff(-1)\n",
    "    df['time_range'] = df['time_range'].apply(lambda x: abs(x))\n",
    "\n",
    "    # resolve any na values to 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the time weighted distribution of liquidity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each asset, get the weighted liquidity dataframe\n",
    "pools = list(asset_subsets.keys())\n",
    "\n",
    "# create a dictionary to store the weighted liquidity dataframes\n",
    "weighted_liquidity_dfs = {}\n",
    "\n",
    "# iterate though each pool and pass to weighted_liquidity function\n",
    "# this will create a dataframe that contains the users # of shares at each timestamp\n",
    "for pool in pools: \n",
    "    df = weighted_liquidity(asset_subsets[pool])\n",
    "    weighted_liquidity_dfs[pool] = df\n",
    "\n",
    "# save the dictionary to a pickle file\n",
    "with open('data/pickles/weighted_liquidity_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(weighted_liquidity_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to take a dataframe, multiply time_period by a scalar, and compute the time weighted liquidity distribution\n",
    "def lp_multiplier(df, timestamp, multiplier):\n",
    "\n",
    "    # set the mask for the time period that we want to apply a multiplier to\n",
    "    mask = (df['timestamp'] <= timestamp)\n",
    "\n",
    "    # multiply the time_range by the multiplier\n",
    "    df.loc[mask, 'time_range'] = df['time_range'] * multiplier\n",
    "\n",
    "    # set the list of users\n",
    "    users = list(df.columns)\n",
    "    users.remove('timestamp')\n",
    "    users.remove('shareCount')\n",
    "    users.remove('time_range')\n",
    "\n",
    "    # now go through each user, calculate their relative proportion of the pool and multiply by the time_range\n",
    "    for user in users:\n",
    "        df[user] = df.apply(lambda x: (x[user] / x['shareCount']) * x['time_range'], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want a function that will return the relative proportion of time weighted risk a user has assumed for a given asset\n",
    "def lp_proportions(df):\n",
    "\n",
    "    # set the list of users\n",
    "    users = list(df.columns)\n",
    "    users.remove('timestamp')\n",
    "    users.remove('shareCount')\n",
    "    users.remove('time_range')\n",
    "\n",
    "    # create a dictionary to store the relative proportions of time weighted risk for each user\n",
    "    proportions = {}\n",
    "    totals = {}\n",
    "    total = 0 \n",
    "\n",
    "    # iterate through each user and get the sum of their relative proportions\n",
    "    for user in users:\n",
    "        val = df[user].sum()\n",
    "        totals[user] = val\n",
    "        total += val\n",
    "\n",
    "    # get a value for the total of all users\n",
    "    for user in users: \n",
    "        proportions[user] = totals[user] / total\n",
    "\n",
    "    return proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [caution] this takes a while to run :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = list(weighted_liquidity_dfs.keys())\n",
    "\n",
    "# create a dictionary to store the final dictionary of relative user weightings for each pool\n",
    "final_lp_distributions = {}\n",
    "\n",
    "# create a dictionary to store the lp_multiplier dataframes\n",
    "lp_multiplier_dfs = {}\n",
    "\n",
    "# go through and for each pool apply the multiplier and get the final proportions\n",
    "for pool in pools:\n",
    "    df = lp_multiplier(weighted_liquidity_dfs[pool], ovm1_timestamp, lp_scalar)\n",
    "    lp_multiplier_dfs[pool] = df\n",
    "    final_lp_distributions[pool] = lp_proportions(df)\n",
    "\n",
    "# save the dictionary to a pickle file\n",
    "with open('data/pickles/lp_multiplier_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(lp_multiplier_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/lp_drops.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_lp_distributions, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### determine user expected rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total amount of rewards that each pool will receive\n",
    "pool_rewards = {}\n",
    "\n",
    "for pool in list(pool_split.keys()):\n",
    "    pool_rewards[pool] = (op_total * lp_percent) * pool_split[pool]\n",
    "\n",
    "pool_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store the expected token distribution for each pool\n",
    "expected_distributions = {}\n",
    "\n",
    "pools = list(final_lp_distributions.keys())\n",
    "\n",
    "user_rewards = {}\n",
    "\n",
    "for pool in pools:\n",
    "\n",
    "    pool_splits = {}\n",
    "\n",
    "    props = final_lp_distributions[pool]\n",
    "\n",
    "    for user in list(props.keys()):\n",
    "        pool_splits[user] = props[user] * pool_rewards[pool]\n",
    "    \n",
    "    user_rewards[pool] = pool_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the lp user_rewards dictionary \n",
    "with open('data/pickles/final_lp_user_rewards.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_rewards, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taker Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in snapshot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from the snapshot\n",
    "trade_df = pd.read_json('data/ovm1/marketSnapshot_ovm1.json')\n",
    "\n",
    "# map to integers\n",
    "trade_df.take_amt = trade_df.take_amt.map(int)\n",
    "trade_df.give_amt = trade_df.give_amt.map(int)\n",
    "\n",
    "# order the dataframe by the timestamp\n",
    "trade_df.sort_values(by='timestamp', ascending=True,inplace=True)\n",
    "\n",
    "# convert the ovm1.0 dataframe columns to matching names \n",
    "trade_df.rename(columns={'taker': 'user', 'take_amt' : 'pay_amt', 'give_amt' : 'buy_amt', 'transactionHash' : 'txn'}, inplace=True)\n",
    "\n",
    "# drop extra columns from ovm1.0 dataset \n",
    "trade_df.drop(['id', 'maker', 'pair'], axis=1, inplace=True)\n",
    "\n",
    "# convert the trade_df datetime object to a timestamp\n",
    "trade_df['timestamp'] = trade_df['timestamp'].apply(lambda x: int(x.timestamp()))\n",
    "\n",
    "# organize the dataframe by column \n",
    "ss_taker_df = trade_df[['user', 'timestamp', 'txn', 'pay_gem', 'pay_amt', 'buy_gem', 'buy_amt']]\n",
    "\n",
    "ss_txns = trade_df.txn.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in dune data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [['user', 'timestamp', 'txn', 'pay_gem', 'pay_amt', 'buy_gem', 'buy_amt']]\n",
    "\n",
    "# load in the csv downloaded from the following dune query: https://dune.com/queries/1134223\n",
    "dune_taker = pd.read_csv('data/dune/taker_ovm1.csv')\n",
    "\n",
    "# convert the timestamp to a unix timestamp\n",
    "dune_taker['timestamp'] = pd.to_datetime(dune_taker['timestamp'])\n",
    "\n",
    "# convert the timestamp to a unix timestamp\n",
    "dune_taker['timestamp'] = dune_taker.timestamp.values.astype(np.int64) // 10**9\n",
    "\n",
    "# for addresses drop the slash and add 0 to the beginning of the string\n",
    "dune_taker['user'] = dune_taker['user'].apply(lambda x: '0' +  x[1:])\n",
    "dune_taker['txn'] = dune_taker['txn'].apply(lambda x: '0' +  x[1:])\n",
    "dune_taker['pay_gem'] = dune_taker['pay_gem'].apply(lambda x: '0' +  x[1:])\n",
    "dune_taker['buy_gem'] = dune_taker['buy_gem'].apply(lambda x: '0' +  x[1:])\n",
    "\n",
    "# convert change values to integers\n",
    "dune_taker['pay_amt'] = dune_taker['pay_amt'].map(int)\n",
    "dune_taker['buy_amt'] = dune_taker['buy_amt'].map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get graph data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query that is used to all logTakes\n",
    "taker_query = '''\n",
    "query allTakes($lastID: ID, $amount: Int, $lastTimestamp: Int) {\n",
    "    takers(first: $amount, where: {id_gt: $lastID, timestamp_gte: $lastTimestamp}) {\n",
    "        id\n",
    "        timestamp\n",
    "        transaction {id}\n",
    "        user {id}\n",
    "        takeAsset {id}\n",
    "        makeAsset {id}\n",
    "        takeAmount\n",
    "        makeAmount\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "## take_asset = input_erc20\n",
    "## make_asset = target_erc20\n",
    "## takeAmount = inputAmount\n",
    "## makeAmount = realizedFill\n",
    "swap_query = '''\n",
    "query allSwaps($lastID: ID, $amount: Int, $lastTimestamp: Int) {\n",
    "    swaps(first: $amount, where: {id_gt: $lastID, timestamp_gte: $lastTimestamp}) {\n",
    "        id\n",
    "        timestamp\n",
    "        user {id}\n",
    "        takeAsset {id}\n",
    "        makeAsset {id}\n",
    "        takeAmount\n",
    "        makeAmount\n",
    "  }\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now query the graph and get the data we need \n",
    "api = 'https://api.thegraph.com/subgraphs/name/denverbaumgartner/devrubiconmarketoptimism'\n",
    "variables1 = {\"lastID\": \"\", \"amount\": 1000, \"lastTimestamp\": 0}\n",
    "variables2 = {\"lastID\": \"\", \"amount\": 1000, \"lastTimestamp\": 0}\n",
    "column1 = 'takers'\n",
    "column2 = 'swaps'\n",
    "takers_df = graph_query(api, taker_query, variables1, column1, id = 'id')\n",
    "swaps_df = graph_query(api, swap_query, variables2, column2, id = 'id')\n",
    "\n",
    "# convert ovm2.0 dataframe columns to matching names\n",
    "takers_df.rename(columns={'takeAmount' : 'pay_amt', 'makeAmount' : 'buy_amt', 'transaction.id' : 'txn', 'user.id' : 'user', 'takeAsset.id' : 'pay_gem', 'makeAsset.id' : 'buy_gem'}, inplace=True)\n",
    "\n",
    "# drop extra columns from ovm2.0 dataset\n",
    "takers_df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# convert the takers_df timestamp from string to integer\n",
    "takers_df['timestamp'] = takers_df['timestamp'].map(int) \n",
    "takers_df['pay_amt'] = takers_df['pay_amt'].map(int)\n",
    "takers_df['buy_amt'] = takers_df['buy_amt'].map(int)\n",
    "\n",
    "# clean up the taker df by column \n",
    "graph_taker_df = takers_df[['user', 'timestamp', 'txn', 'pay_gem', 'pay_amt', 'buy_gem', 'buy_amt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge in taker datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ss_taker_df, dune_taker, graph_taker_df]\n",
    "\n",
    "# concat them all togehther into a single dataframe\n",
    "main_taker_df = pd.concat(datasets)\n",
    "\n",
    "# clean up the merged dataframe, reset the index, drop any potential duplicates, and sort by timestamp\n",
    "main_taker_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print the before and after size of the dataframe\n",
    "main_taker_df.drop_duplicates(inplace=True)\n",
    "\n",
    "main_taker_df.sort_values(by=['timestamp'], inplace=True)\n",
    "\n",
    "# map the pay_amt and buy_amt to an integer for later calculations\n",
    "main_taker_df.pay_amt = main_taker_df.pay_amt.map(int)\n",
    "main_taker_df.buy_amt = main_taker_df.buy_amt.map(int)\n",
    "\n",
    "# iterate through the taker_df and conver the pay_amt and buy_amt to decimals\n",
    "for index, row in main_taker_df.iterrows():\n",
    "    main_taker_df.loc[index, 'pay_amt'] = main_taker_df.loc[index, 'pay_amt'] / (10**op_main_asset_decimals[main_taker_df.loc[index, 'pay_gem']])\n",
    "    main_taker_df.loc[index, 'buy_amt'] = main_taker_df.loc[index, 'buy_amt'] / (10**op_main_asset_decimals[main_taker_df.loc[index, 'buy_gem']])\n",
    "\n",
    "# convert the asset addresses to symbols \n",
    "main_taker_df['pay_gem'] = main_taker_df['pay_gem'].map(op_main_asset_names)\n",
    "main_taker_df['buy_gem'] = main_taker_df['buy_gem'].map(op_main_asset_names)\n",
    "\n",
    "# if the pay_gem is the stable quote set USD value to pay_amt, else set it to buy_amt\n",
    "main_taker_df['USD'] = main_taker_df.apply(lambda x: x.pay_amt if x.pay_gem in ['USDC', 'USDT', 'DAI'] else x.buy_amt, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map newest swap router txns to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary that maps the id (txn hash) to the user.id\n",
    "txn_to_user = {}\n",
    "\n",
    "for index, row in swaps_df.iterrows():\n",
    "    txn_to_user[swaps_df.loc[index, 'id']] = swaps_df.loc[index, 'user.id']\n",
    "\n",
    "# within the taker_df map the transaction.id to the \n",
    "for index, row in main_taker_df.iterrows():\n",
    "    if row['txn'] in txn_to_user:\n",
    "        main_taker_df.loc[index, 'user'] = txn_to_user[main_taker_df.loc[index, 'txn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map old swap router txns back to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps_df = main_taker_df[main_taker_df['user'] == '0x45969104ef4561cee269b334d8cb7a99206a09e5']\n",
    "txns = list(swaps_df['txn'].unique())\n",
    "\n",
    "router_swaps = []\n",
    "\n",
    "for file in os.listdir('data/router/'):\n",
    "\n",
    "    update = pd.read_csv('data/router/' + file)\n",
    "\n",
    "    router_swaps.append(update)\n",
    "\n",
    "agg_df = pd.concat(router_swaps)\n",
    "agg_df = agg_df.drop_duplicates()\n",
    "\n",
    "# go through an map the transaction hash to the original swap user \n",
    "mapped = {}\n",
    "\n",
    "for txn in txns: \n",
    "    try:\n",
    "        mapped[txn] = list(agg_df[agg_df['Txhash'] == txn]['From'])[0]\n",
    "    except: \n",
    "        print(txn)\n",
    "\n",
    "# for all transactions that the taker is the router, map back to the original user \n",
    "# change this to be a lamda function\n",
    "txnst = list(mapped.keys())\n",
    "\n",
    "for index, row in main_taker_df.iterrows():\n",
    "    if row['txn'] in txnst:\n",
    "        main_taker_df.loc[index, 'user'] = mapped[main_taker_df.loc[index, 'txn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no log take is still mapped back to the old router\n",
    "main_taker_df[main_taker_df['user'] == '0x45969104ef4561cee269b334d8cb7a99206a09e5'].sort_values(by=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any trades that have occured after the proposal was released\n",
    "taker_df = main_taker_df[main_taker_df['timestamp'] <= cutoff_timestamp]\n",
    "\n",
    "# convert the user column to lowercase\n",
    "taker_df['user'] = taker_df['user'].str.lower()\n",
    "\n",
    "# drop any rows that have a user in the exclude_list\n",
    "taker_df = taker_df[~taker_df['user'].isin(exclude_list)]\n",
    "\n",
    "# reset the index\n",
    "taker_df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter addresses that exchanged < $10 in total volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out transactions from users that had less than $10 USD\n",
    "volume_cutoff = 10\n",
    "cutoff_list = []\n",
    "\n",
    "users = list(taker_df.user.unique())\n",
    "\n",
    "for user in users:\n",
    "        user_volume = taker_df[taker_df['user'] == user].USD.sum()\n",
    "        if user_volume < volume_cutoff:\n",
    "            cutoff_list.append(user)\n",
    "\n",
    "# filter out transactions from users in the cutoff_list\n",
    "taker_df = taker_df[~taker_df['user'].isin(cutoff_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the taker_df to a pickle file\n",
    "taker_df.to_pickle('data/pickles/taker_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### determine relative user proportion of volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to take a dataframe and apply a multiplier for values on or before a certain timestamp\n",
    "def volume_mutiplier(df, timestamp, multiplier):\n",
    "\n",
    "    # set the mask for the time period that we want to apply a multiplier to\n",
    "    mask = (df['timestamp'] <= timestamp)\n",
    "\n",
    "    # apply the multiplier to the values in the mask\n",
    "    df.loc[mask, 'USD'] = df.loc[mask, 'USD'] * multiplier\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to find a user's proportion of total volume\n",
    "def user_proportion(df): \n",
    "    user_prop = {}\n",
    "    user_volumes = {}\n",
    "    users = list(df.user.unique())\n",
    "\n",
    "    # total volume \n",
    "    total_volume = df.USD.sum()\n",
    "\n",
    "    # go through each user and calculate their cumulative volume\n",
    "    for user in users:\n",
    "        user_volumes[user] = df[df.user == user].USD.sum()\n",
    "        user_prop[user] = user_volumes[user] / total_volume\n",
    "\n",
    "    return user_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the relative volume of each user\n",
    "df = taker_df\n",
    "\n",
    "df = volume_mutiplier(df, ovm1_timestamp, taker_scalar)\n",
    "user_prop = user_proportion(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how much op each user will get \n",
    "users = list(user_prop.keys())\n",
    "\n",
    "taker_coins = op_total * taker_percent\n",
    "\n",
    "taker_drops = {}\n",
    "\n",
    "for user in users:\n",
    "    taker_drops[user] = taker_coins * user_prop[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the op drops to a pickle file\n",
    "with open('data/pickles/taker_drops.pickle', 'wb') as handle:\n",
    "    pickle.dump(taker_drops, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the taker_drops pickle file\n",
    "with open('data/pickles/taker_drops.pickle', 'rb') as handle:\n",
    "    taker_drops = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickles/final_lp_user_rewards.pickle', 'rb') as handle:\n",
    "    lp_drops = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = 0 \n",
    "\n",
    "# print out the total drops for volume, and each pool \n",
    "total_drops = sum(taker_drops.values())\n",
    "overall += total_drops\n",
    "print('expected takeer drop:' + str(op_total*taker_percent))\n",
    "print('Total Drops: ' + str(total_drops))\n",
    "\n",
    "for pool in list(lp_drops.keys()):\n",
    "    print('expected airdrop for ' + pool + ': ' + str(op_total*lp_percent*pool_split[pool]))\n",
    "    print('Airdrop for ' + pool + ': ' + str(sum(lp_drops[pool].values())))\n",
    "    overall += sum(lp_drops[pool].values())\n",
    "\n",
    "print('expected drops for all pools: ' + str(op_total))\n",
    "print('Total Drops: ' + str(overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store the users total airdrop in \n",
    "DropTopWop = {}\n",
    "\n",
    "for key in list(lp_drops.keys()):\n",
    "    for user in list(lp_drops[key].keys()):\n",
    "        try:\n",
    "            DropTopWop[user] += lp_drops[key][user]\n",
    "        except:\n",
    "            DropTopWop[user] = lp_drops[key][user]\n",
    "\n",
    "for kiy in list(taker_drops.keys()):\n",
    "    try:\n",
    "        DropTopWop[kiy] += taker_drops[kiy]\n",
    "    except:\n",
    "        DropTopWop[kiy] = taker_drops[kiy]\n",
    "\n",
    "#save the DropTopWop dictionary to csv\n",
    "drop = pd.DataFrame(list(DropTopWop.items()))\n",
    "drop.to_csv('rubicon-op-summer-airdrop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(DropTopWop.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OP Airdrop Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_total = 135000\n",
    "cutoff_timestamp = 1653339960\n",
    "ovm1_timestamp = 1636696800\n",
    "lp_percent = 0.69\n",
    "taker_percent = 0.31\n",
    "lp_scalar = 2.25\n",
    "taker_scalar = 1.5\n",
    "volume_cutoff = 10\n",
    "pool_split = {'ETH': .325, 'WBTC': .1, 'USDC': .275, 'USDT': .1, 'DAI': .15, 'SNX': .05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_lps = pd.read_pickle('data/pickles/lp_df.pkl')\n",
    "eligble_traders = pd.read_pickle('data/pickles/taker_df.pkl')\n",
    "airdrop = pd.read_csv('rubicon-op-summer-airdrop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how much of the airdrop went to ovm 1.0 users\n",
    "ovm1_eligible_lps = eligible_lps[eligible_lps['timestamp'] <= ovm1_timestamp]\n",
    "ovm1_eligible_traders = eligble_traders[eligble_traders['timestamp'] <= ovm1_timestamp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of users who were eligible for the airdrop in ovm 1.0\n",
    "ovm1_users = list(ovm1_eligible_lps.user.unique())\n",
    "ovm1_users.extend(list(ovm1_eligible_traders.user.unique()))\n",
    "\n",
    "# subset the airdrop dataframe to only include users who were eligible for the airdrop in ovm 1.0\n",
    "ovm1_airdrop = airdrop[airdrop['0'].isin(ovm1_users)]\n",
    "\n",
    "# rename column 0 to user and column 1 to OP\n",
    "ovm1_airdrop.rename(columns={'0': 'user', '1': 'OP'}, inplace=True)\n",
    "airdrop.rename(columns={'0': 'user', '1': 'OP'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the proportion of the airdrop that went to ovm 1.0 users, convert decimals to percentages\n",
    "print('proportion of airdrop that went to eligible ovm 1.0 users: ' + str(round((ovm1_airdrop.OP.sum() / airdrop.OP.sum()), 4) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42703353d823f48c25e752b895a3a714ff45c5e6c59b9c3c72d0667016a5ab05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
